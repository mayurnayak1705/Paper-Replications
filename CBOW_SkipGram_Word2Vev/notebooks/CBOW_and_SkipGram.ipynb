{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install --upgrade torch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xNsBFN33YMIi",
        "outputId": "e9296bc7-6fc9-497e-d3c1-4c40a246ac0f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.7.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.11/dist-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.11/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.11/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.11/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in /usr/local/lib/python3.11/dist-packages (from torch) (9.5.1.17)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.11/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.11/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.11/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.11/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.3)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in /usr/local/lib/python3.11/dist-packages (from torch) (2.26.2)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.11/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.11/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.11/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.3.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.3.0)\n",
            "Requirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.11/dist-packages (from triton==3.3.0->torch) (75.2.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sERpQtG8Ip6M"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import re"
      ],
      "metadata": {
        "id": "16E7VsPBLsoo"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"/content/shakespeare.txt\", \"r\" ,encoding=\"utf8\") as f:\n",
        "  text = f.read()\n",
        "text = text.lower()"
      ],
      "metadata": {
        "id": "OUL0L_nFMQbU"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = re.findall(r\"\\w+|[^\\w\\s]\", text)"
      ],
      "metadata": {
        "id": "aGhNeQj5NfYb"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = sorted(set(tokens))"
      ],
      "metadata": {
        "id": "JLrAs6X4Ngy7"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word2idx = {w: i for i, w in enumerate(vocab)}\n",
        "idx2word = {i: w for w, i in word2idx.items()}\n",
        "vocab_size = len(vocab)"
      ],
      "metadata": {
        "id": "wrpjX1ktNlBp"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "indices = [word2idx[w] for w in tokens]"
      ],
      "metadata": {
        "id": "1bZ2OQ4qNm6p"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CBOW_Dataset(Dataset):\n",
        "    def __init__(self, data, context_size):\n",
        "        self.data = data\n",
        "        self.context_size = context_size\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data) - 2 * self.context_size\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # shift index so you always have full context on both sides\n",
        "        center = idx + self.context_size\n",
        "        context = self.data[idx:center] + self.data[center+1:center+1+self.context_size]\n",
        "        target = self.data[center]\n",
        "        return torch.tensor(context, dtype=torch.long), torch.tensor(target, dtype=torch.long)"
      ],
      "metadata": {
        "id": "jgUiCouYNoUB"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "context_size = 2\n",
        "dataset_CBOW = CBOW_Dataset(indices, context_size)\n",
        "dataloader = DataLoader(dataset_CBOW, batch_size=64, shuffle=True)\n"
      ],
      "metadata": {
        "id": "8FjEha1vQk5k"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "\n",
        "class CBOW_Model(nn.Module):\n",
        "  def __init__(self, vocab_size, embedding_dim):\n",
        "    super(CBOW_Model, self).__init__()\n",
        "    self.embeddings  = nn.Embedding(vocab_size, embedding_dim)\n",
        "    self.linear =  nn.Linear(embedding_dim, vocab_size)\n",
        "\n",
        "  def forward(self, x):\n",
        "    embedd = self.embeddings(x)\n",
        "    embedd = embedd.mean(axis=1)\n",
        "    output = self.linear(embedd)\n",
        "    return output"
      ],
      "metadata": {
        "id": "jm-gHihxQ69z"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cbow_model = CBOW_Model(vocab_size, 100)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(cbow_model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "ez-oNJUmXAfY"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(5):\n",
        "    total_loss = 0\n",
        "    for context, target in dataloader:\n",
        "        optimizer.zero_grad()\n",
        "        output = cbow_model(context)\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    print(f\"Epoch {epoch + 1}, Loss: {total_loss:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bBzLFoueXarj",
        "outputId": "654de897-0b1a-4ff6-d618-1243b41aeb7c"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 24978.7266\n",
            "Epoch 2, Loss: 21353.4814\n",
            "Epoch 3, Loss: 20097.2214\n",
            "Epoch 4, Loss: 19228.9405\n",
            "Epoch 5, Loss: 18549.2545\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SkipGram_Dataset(Dataset):\n",
        "    def __init__(self, data, context_size):\n",
        "        self.pairs = []\n",
        "        for i in range(context_size, len(data) - context_size):\n",
        "            center = data[i]\n",
        "            context = data[i - context_size:i] + data[i + 1:i + context_size + 1]\n",
        "            for target in context:\n",
        "                self.pairs.append((center, target))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.pairs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        center, target = self.pairs[idx]\n",
        "        return torch.tensor(center, dtype=torch.long), torch.tensor(target, dtype=torch.long)"
      ],
      "metadata": {
        "id": "-JRHSiU9XguI"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "context_size = 2\n",
        "dataset_SkipGram = SkipGram_Dataset(indices, context_size)\n",
        "dataloader_SkipGram = DataLoader(dataset_SkipGram, batch_size=64, shuffle=True)"
      ],
      "metadata": {
        "id": "8ULqS080imfn"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "\n",
        "class SkipGram_Model(nn.Module):\n",
        "  def __init__(self, vocab_size, embedding_dim):\n",
        "    super(SkipGram_Model, self).__init__()\n",
        "    self.embeddings  = nn.Embedding(vocab_size, embedding_dim)\n",
        "    self.linear =  nn.Linear(embedding_dim, vocab_size)\n",
        "\n",
        "  def forward(self, x):\n",
        "    # print(x.shape)\n",
        "    embedd = self.embeddings(x)\n",
        "    # print(embedd.shape)\n",
        "    output = self.linear(embedd)\n",
        "    # print(output.shape)\n",
        "    return output"
      ],
      "metadata": {
        "id": "J7pWXugfim_f"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "skipgram_model = SkipGram_Model(vocab_size, 100)\n",
        "criterion_ = nn.CrossEntropyLoss()\n",
        "optimizer_ = torch.optim.Adam(skipgram_model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "vk6KnHYOn1KJ"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(5):\n",
        "    total_loss = 0\n",
        "    for context, target in dataloader_SkipGram:\n",
        "        optimizer_.zero_grad()\n",
        "        output = skipgram_model(context)\n",
        "        loss = criterion_(output, target)\n",
        "        loss.backward()\n",
        "        optimizer_.step()\n",
        "        total_loss += loss.item()\n",
        "    print(f\"Epoch {epoch + 1}, Loss: {total_loss:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wx-w24csoHbi",
        "outputId": "bce7cf66-b988-4aa7-bed8-33e3492f7e91"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 96526.6824\n",
            "Epoch 2, Loss: 95613.5060\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Xi9I21ZyoSiz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}