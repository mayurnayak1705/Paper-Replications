{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "DjVH6tuYfg3t"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"/content/shakespeare.txt\", \"r\" ,encoding=\"utf8\") as f:\n",
        "  text = f.read()\n",
        "text = text.lower()"
      ],
      "metadata": {
        "id": "xEjNzXOchrBc"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = re.findall(r\"\\w+|[^\\w\\s]\", text)"
      ],
      "metadata": {
        "id": "zdx2gVcdhsai"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = sorted(set(tokens))"
      ],
      "metadata": {
        "id": "2HMuJtr5huH1"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word2idx = {w: i for i, w in enumerate(vocab)}\n",
        "idx2word = {i: w for w, i in word2idx.items()}\n",
        "vocab_size = len(vocab)"
      ],
      "metadata": {
        "id": "Na6LI9QHhz1u"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "indices = [word2idx[w] for w in tokens]"
      ],
      "metadata": {
        "id": "jhSqeVgph1hP"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader"
      ],
      "metadata": {
        "id": "Fuz6DOk1h5tG"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RNNLM_Dataset(Dataset):\n",
        "    def __init__(self, data, context_size):\n",
        "        self.data = data\n",
        "        self.context_size = context_size\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data) - self.context_size\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "      x = self.data[idx : idx + self.context_size]          # Input sequence\n",
        "      y = self.data[idx + 1 : idx + self.context_size + 1]  # Next-word targets\n",
        "      return torch.tensor(x), torch.tensor(y)\n"
      ],
      "metadata": {
        "id": "lu_j6LP7h7bL"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "context_size = 15\n",
        "dataset = RNNLM_Dataset(indices, context_size)\n",
        "dataloader = DataLoader(dataset, batch_size=64, shuffle=True)"
      ],
      "metadata": {
        "id": "x6akuRVJiJ2Z"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "class RNNLM(nn.Module):\n",
        "  def __init__(self, vocab_size, hidden_size=128, embed_dim=128, num_layers=1):\n",
        "    super().__init__()\n",
        "    self.embedding = nn.Embedding(vocab_size,embed_dim)\n",
        "    self.rnn = nn.RNN(embed_dim, hidden_size,num_layers,batch_first=True)\n",
        "    self.fc = nn.Linear(hidden_size,vocab_size)\n",
        "\n",
        "  def forward(self, x, hidden):\n",
        "    emb = self.embedding(x)\n",
        "    out,hidden = self.rnn(emb,hidden)\n",
        "    logits = self.fc(out)\n",
        "    return logits,hidden\n",
        "\n",
        "  def init_hidden(self, batch_size):\n",
        "    # Initialize hidden state to zeros\n",
        "    weight = next(self.parameters())\n",
        "    return torch.zeros(self.rnn.num_layers, batch_size,self.rnn.hidden_size, device=weight.device)\n"
      ],
      "metadata": {
        "id": "v8XCvwVdiShf"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = RNNLM(vocab_size, 128, 128, 1)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "batch_size = 64"
      ],
      "metadata": {
        "id": "sxbfhPCKqU-g"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(1,5):\n",
        "  total_loss = 0.0\n",
        "  hidden = model.init_hidden(batch_size)\n",
        "  for x_batch,y_batch in dataloader:\n",
        "    optimizer.zero_grad()\n",
        "    logits, hidden = model(x_batch, hidden.detach())\n",
        "    logits = logits.view(-1, vocab_size)\n",
        "    y_batch = y_batch.view(-1)\n",
        "    loss = criterion(logits, y_batch)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    total_loss += loss.item()\n",
        "    avg = total_loss / len(dataloader)\n",
        "  print(f\"Epoch {epoch}/{10}, Loss: {avg:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vyJFgzk_qcSH",
        "outputId": "f82509cf-4a1b-439e-e748-0777e3b84613"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Loss: 4.6980\n",
            "Epoch 2/10, Loss: 3.9143\n",
            "Epoch 3/10, Loss: 3.5542\n",
            "Epoch 4/10, Loss: 3.3357\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dZx7MeHVjbgk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}